<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Image Deblurring using DeblurGAN</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="Report_files/libs/clipboard/clipboard.min.js"></script>
<script src="Report_files/libs/quarto-html/quarto.js"></script>
<script src="Report_files/libs/quarto-html/popper.min.js"></script>
<script src="Report_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="Report_files/libs/quarto-html/anchor.min.js"></script>
<link href="Report_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="Report_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="Report_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="Report_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="Report_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">


</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Image Deblurring using DeblurGAN</h1>
<p class="subtitle lead">DSAN 6500 Project</p>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Nandini Kodali </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Georgetown University
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
  
    
  </div>
  


</header>


<section id="introduction" class="level1">
<h1>1. Introduction</h1>
<p>Image deblurring is a critical task in low-level computer vision that aims to restore sharp, high-quality images from those corrupted by motion blur or defocus blur. These distortions commonly arise in dynamic environments, where camera shake, object motion, or low shutter speeds can severely degrade image quality. Restoring such images is essential not only for aesthetic purposes but also for the performance of downstream applications such as object detection, scene understanding, and autonomous navigation.</p>
<p>Traditional deblurring models rely on explicit physical models of blur formation and handcrafted priors, such as total variation or edge-preserving smoothness constraints. However, these approaches often struggle with generalization, especially in the presence of complex, spatially-varying blur.</p>
<p>In recent years, Generative Adversarial Networks (GANs) have emerged as a powerful paradigm for image restoration tasks. These models outperform conventional methods by learning rich priors directly from data, avoiding the need for handcrafted blur kernels.</p>
<p>This project presents a DeblurGAN-style architecture trained on GoPro data, leveraging both Wasserstein loss and VGG-based perceptual loss to preserve high-level semantic features. The generator learns to map blurred images to their sharp counterparts, while the discriminator enforces photorealistic fidelity at the patch level using a PatchGAN approach.</p>
</section>
<section id="dataset" class="level1">
<h1>2. Dataset</h1>
<p>This project used the GoPro_large dataset, which contains paired blurred and sharp images specially curated for training and evaluating image deblurring models. The dataset contains 2,103 training pairs and 1,111 testing pairs, with each pair containing a blurred image and its corresponding ground-truth sharp image. All images are originally in high resolution (1280x720 pixels) and are organized into seperate folders for blurred and sharp samples.</p>
<p>To make the dataset suitable for training, all images were resized to 256x256 pixels. Additionally, the pixel values were normalized to the range [-1,1].</p>
</section>
<section id="model-architecture" class="level1">
<h1>3. Model Architecture</h1>
<p>The proposed solution adopts a GAN framework for image deblurring, consisting of two main components: a Generator and a Discriminator. The generator is responsible for restoring a sharp image from a blurred input, while the discriminator attempts to distinguish between real sharp images and those generated by the model.</p>
<section id="generator" class="level2">
<h2 class="anchored" data-anchor-id="generator">3.1. Generator</h2>
<p>The generator takes a blurred image as input and outputs a deblurred version of the same resolution. Key features of the generator include:</p>
<ul>
<li><strong>Reflection Padding</strong>: Applied before convolutions to preserve edge information and avoid artifacts introduced by zero-padding.</li>
<li><strong>Downsampling Layers</strong>: Two convolutional layers with stride 2 are used to progressively reduce the spatial dimensions while increasing the depth of feature maps.</li>
<li><strong>Residual Blocks</strong>: A sequence of 9 residual blocks allows the network to learn complex transformations while preserving spatial coherence. Each block consists of two convolutional layers, each preceded by reflection padding, followed by batch normalization and ReLU activation.</li>
<li><strong>Upsampling Layers</strong>: Two stages of nearest-neighbor upsampling are followed by 3x3 convolutions to restore image resolution.</li>
<li><strong>Output Layer</strong>: A final convolution with tanh activation maps the output to the range [-1,1]. A residual connection is added from the input to the output, and the result is scaled to [-1,1] using a Lambda layer.</li>
</ul>
</section>
<section id="discriminator" class="level2">
<h2 class="anchored" data-anchor-id="discriminator">3.2. Discriminator</h2>
<p>The discriminator is modeled after the PatchGAN architecture. Instead of evaluating the entire image holistically, it divides the input into small patches and classifies each patch as real or fake. This encourages the generator to produce fine-grained realistic details.</p>
<p>The discriminator takes either a real sharp image or a deblurred image generated by the generator as the input and processes it through five convolutional layers. The number of filters progressively increases. Each convolutional layer is followed by a LeakyReLU activation function and Batch Normalization, except for the first layer. Instead of outputting a single real/fake classification, the network produces a grid of patch-level predictions, capturing local realism across different regions of the image. This grid is then aggregated and passed through a final dense layer to produce a single scalar value representing the overall realism of the input image.</p>
</section>
<section id="loss-functions" class="level2">
<h2 class="anchored" data-anchor-id="loss-functions">3.3. Loss Functions</h2>
<p><strong>Perceptual Loss</strong>: Perceptual loss is designed to capture high-level semantic differences between images, as opposed to pixel-wise losses like MSE which often result in blurry outputs. In this project, the perceptual loss is computed by passing both the generated (deblurred) image and the target sharp image through a pre-trained VGG16 network and extracting feature maps from an intermediate layer (<code>block3_conv3</code>). The loss is then calculated as the mean squared error between these feature representations. This encourages the generator to produce outputs that are not only visually sharp but also structurally and semantically similar to the target image, leading to more realistic and perceptually pleasing results.</p>
<p><strong>Wasserstein Loss</strong>: Wasserstein loss addresses several shortcomings of the traditional binary cross-entropy loss used in GANs. It provides a continuous and meaningful gradient even when the discriminator performs well, avoiding vanishing gradients. This makes GAN training more stable and improves convergence. In this implementation, the discriminator is trained to output high values for real images and low values for fake (generated) images. The generator aims to generate outputs that are indistinguishable from real images in the eyes of the discriminator. In the context of Wasserstein loss, this translates to minimizing the negative of the discriminator’s output for the generated images. As the generator improves, the Wasserstein distance between the distributions of real and fake images reduces, ultimately pushing the generator to produce more realistic and sharper images. This adversarial dynamic helps guide the generator toward creating high-quality deblurred outputs.</p>
</section>
</section>
<section id="training" class="level1">
<h1>4. Training</h1>
<p>The model was trained for 200 epochs using a batch size of 1. Training followed a standard GAN paradigm where the generator and discriminator were optimized alternately.</p>
<div style="text-align: center;">
<img src="Images/training_loss_curves.png" alt="Training Loss" style="max-width: 80%;"><br>
<p style="font-size: 0.85em; color: #555; margin-top: 0.5em;">
<strong>Figure 1:</strong> Generator loss steadily decreases throughout training, indicating consistent improvement.
</p>
</div>
<p>However, GAN loss values alone do not provide a complete picture of model performance, especially in tasks like image restoration where perceptual quality is crucial. Therefore, to determine the best model checkpoint, we evaluated outputs from various epochs using Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM) — two standard metrics for assessing image quality.</p>
<div style="text-align: center;">
<img src="Images/PSNR.png" alt="PSNR Trend" style="max-width: 80%;"><br>
<p style="font-size: 0.85em; color: #555; margin-top: 0.5em;">
<strong>Figure 2:</strong> PSNR increases steadily over the training epochs, reflecting improved reconstruction accuracy.
</p>
</div>
<div style="text-align: center;">
<img src="Images/SSIM.png" alt="SSIM Trend" style="max-width: 80%;"><br>
<p style="font-size: 0.85em; color: #555; margin-top: 0.5em;">
<strong>Figure 3:</strong> SSIM also improves with training, capturing better structural and perceptual similarity.
</p>
</div>
<p>From the plots, we observe that both PSNR and SSIM values peak around epoch 165, after which there is a slight decline. This suggests that continued training beyond this point yields diminishing returns and may risk overfitting. As a result, the model from epoch 165 was selected as the final checkpoint for testing.</p>
</section>
<section id="results" class="level1">
<h1>5. Results</h1>
<div style="text-align: center;">
<img src="Images/test1.png" alt="Training Loss" style="max-width: 80%;"><br>
<p style="font-size: 0.85em; color: #555; margin-top: 0.5em;">
<strong>Figure 4</strong>
</p>
</div>
<div style="text-align: center;">
<img src="Images/test3.png" alt="Training Loss" style="max-width: 80%;">
</div>
<div style="text-align: center;">
<img src="Images/test2.png" alt="Training Loss" style="max-width: 80%;"><br>
<p style="font-size: 0.85em; color: #555; margin-top: 0.5em;">
<strong>Figure 5</strong>
</p>
</div>
<p>To assess the effectiveness of the proposed deblurring approach, we compare our results with other deblurring models. The table below summarizes the PSNR and SSIM achieved by various techniques. Our model achieves higher PSNR than traditional CNN-based approaches and comparable SSIM values, highlighting its ability to produce sharper and perceptually realistic outputs.</p>
<table class="table">
<caption><strong>Table 1</strong></caption>
<colgroup>
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Sun <em>et al.</em></th>
<th style="text-align: center;">Nah <em>et al.</em></th>
<th style="text-align: center;">Xu <em>et al.</em></th>
<th style="text-align: center;">DeblurGAN</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><strong>PSNR</strong></td>
<td style="text-align: center;">24.6</td>
<td style="text-align: center;">28.3</td>
<td style="text-align: center;">25.1</td>
<td style="text-align: center;">26.42</td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>SSIM</strong></td>
<td style="text-align: center;">0.84</td>
<td style="text-align: center;">0.916</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;">0.87</td>
</tr>
</tbody>
</table>
</section>
<section id="conclusion" class="level1">
<h1>6. Conclusion</h1>
<p>This project explored a deep learning-based solution to the problem of motion deblurring using a Generative Adversarial Network (GAN) architecture trained on the GoPro dataset. By incorporating adversarial training with a perceptual loss derived from intermediate features of a pre-trained VGG16 network, the model successfully generated sharp and visually realistic deblurred images. The generator, designed with residual blocks and reflection padding, was trained alongside a PatchGAN discriminator to jointly learn global image consistency and restore fine textures, enabling the production of sharp and perceptually realistic outputs. The model achieved a Peak Signal-to-Noise Ratio (PSNR) of 26.42 and a Structural Similarity Index (SSIM) of 0.87, demonstrating strong reconstruction capability.</p>
<p>However, while the PSNR values indicate good pixel-level accuracy, the SSIM suggests that there is still room for improvement in preserving perceptual and structural similarity. One promising direction for future work is to refine the perceptual loss component by extracting features from multiple layers of the VGG16 network. This could provide a richer, more hierarchical understanding of the image content, encouraging the generator to better reconstruct both low-level textures and high-level semantics. Further improvements could involve experimenting with deeper or more expressive feature extractors like VGG19 or ResNet, which may help enhance structural fidelity and elevate SSIM scores.</p>

</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-brownlee2020wasserstein" class="csl-entry" role="listitem">
Brownlee, Jason. 2020. <span>“How to Implement Wasserstein Loss for Generative Adversarial Networks.”</span> <a href="https://machinelearningmastery.com/how-to-implement-wasserstein-loss-for-generative-adversarial-networks/" class="uri">https://machinelearningmastery.com/how-to-implement-wasserstein-loss-for-generative-adversarial-networks/</a>.
</div>
<div id="ref-demir1803patch" class="csl-entry" role="listitem">
Demir, U, and G Unal. n.d. <span>“Patch-Based Image Inpainting with Generative Adversarial Networks. arXiv 2018.”</span> <em>arXiv Preprint arXiv:1803.07422</em>.
</div>
<div id="ref-kupyn2018deblurgan" class="csl-entry" role="listitem">
Kupyn, Orest, Volodymyr Budzan, Mykola Mykhailych, Dmytro Mishkin, and Jiřı́ Matas. 2018. <span>“Deblurgan: Blind Motion Deblurring Using Conditional Adversarial Networks.”</span> In <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>, 8183–92.
</div>
<div id="ref-gopro2023dataset" class="csl-entry" role="listitem">
Lqzmlaq. 2023. <span>“GoPro Large Dataset.”</span> <a href="https://www.kaggle.com/datasets/lqzmlaq/gopro-large" class="uri">https://www.kaggle.com/datasets/lqzmlaq/gopro-large</a>.
</div>
<div id="ref-nah2017deep" class="csl-entry" role="listitem">
Nah, Seungjun, Tae Hyun Kim, and Kyoung Mu Lee. 2017. <span>“Deep Multi-Scale Convolutional Neural Network for Dynamic Scene Deblurring.”</span> In <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>, 3883–91.
</div>
<div id="ref-kerasvggcode2024" class="csl-entry" role="listitem">
Team, Keras. 2024. <span>“Keras VGG16 Implementation (V3.9.2).”</span> <a href="https://keras.io/api/applications/vgg/" class="uri">https://keras.io/api/applications/vgg/</a>.
</div>
<div id="ref-xu2013unnatural" class="csl-entry" role="listitem">
Xu, Li, Shicheng Zheng, and Jiaya Jia. 2013. <span>“Unnatural L0 Sparse Representation for Natural Image Deblurring.”</span> In <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>, 1107–14.
</div>
</div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>